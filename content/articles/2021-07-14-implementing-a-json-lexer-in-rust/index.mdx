---
title: Implementing A JSON Lexer In Rust
published: 2021-07-14
category: compiler
tags:
  - tutorial
  - lexer
  - rust
---

So.. you're wondering how to easily do lexical analysis on a stream of text? I
got you. We will cover simple tokenization, sufficient to handle small DSLs,
config files etc.

## What Is Lexical Analysis?

If you're new to language design and compiler / interpreter development you may
ask yourself what lexing (or lexical analysis) is; The answer is quite simple -
basically every language implementation is (or at least should be) split up
into multiple stages of processing. Lexing all comes down to really split
up a stream of meaningless characters into small logical units called tokens.
What these are heavily depends on your context. For example in rust a token
could be an identifier, a number, a keyword or a parenthesis.

Lexical analysis is usually the first thing to do, no matter what we want to
accomplish afterwards.

## Syntax Rules

So now that we know that a lexer tries to cut the stream into tokens, we
need to define a set of valid tokens and syntax rules for these tokens right?
This part usually is a part of the "grammar" of your language, but thats a
thing we wont further dig into in this post.

Before we can start hacking on the implementation we should carefully think
about what we want to accomplish and how our syntax looks like.

### JSON

For the sake of simplicity we'll use the JSON specification, since nearly
everyone has already worked with it and it's quite easy to do in a single post.

The next question should be: What makes up valid JSON? We have only 6 elements
in the language which you can combine in arbitrary ways to get a valid JSON
blob:

- Null
- Booleans
- Numbers
- Strings
- Arrays
- Objects

**But apparently these syntax elements do not resolve 1 to 1 into tokens!**
Remember: Lexing is happening on a stream of chars and we have very
little knowledge about the context of our chars. So "wrapping types" like
arrays, objects (or for other languages: anything that has a beginning and an
ending) is resolved to the smallest unit we can inspect: The opening and
closing parenthesis. So our array parens are `[` and `]` and for objects they
are `{` and `}`. Furthermore we have the element delimiter symbol (used both in
arrays and objects) `,` and the object key delimiter `:`.

> Lexing is not about inspecting the relation between the occurence of
> the single tokens at all! So an invalid stream may be tokenized very well.
> The occurence order is validated by another step in the compiler pipeline
> called a parser.

So we've already got 6 Tokens to handle the two complex composition types, what
more do we have?

##### Null
Obviously a fixed sequence of chars.

##### Booleans
Same as above.

##### Numbers
A series of digits (without leading zeros!) optionally prefixed by a `-`,
optionally delimited by a `.`.  At the end of a number literal we can specify
an exponent with `e` or `E` followed by an optional sign `+` or `-` and  a
series of digits (may contain leading zeros).

##### Strings
With dynamic content which is enclosed by `"` and can contain escaped
chars (e.g. `\"`).

### JSON Tokens

To keep an overview of what we have found so far i'll list all tokens up down
below:

```
OpeningBracket
ClosingBracket
OpeningCurlyBracket
OpeningCurlyBracket
ElementDelimiter
KeyDelimiter
Null
Boolean
Number
String
```

Since all of these hold a bit of data about the thing they contain (e.g. a
boolean must hold the information if it is `true` / `false`) we can start
writing a bit of rust to make that as clear as possible:


```rust {"file": "src/token.rs"}
use crate::types::*;

#[derive(Clone, Debug, PartialEq)]
pub enum Token {
    Bracket(ParenthesisType),
    CurlyBracket(ParenthesisType),
    ElementDelimiter,
    KeyDelimiter,
    Null,
    Boolean(JSONBoolean),
    Number(JSONNumber),
    String(JSONString),
}

#[derive(Clone, Debug, PartialEq)]
pub enum ParenthesisType {
    Open,
    Close,
}
```

```rust {"file": "src/types.rs"}
pub type JSONBoolean = bool;
pub type JSONString = String;

#[derive(Clone, Copy, Debug, PartialEq)]
pub struct JSONNumber {
    mantissa: f64,
    exponent: Option<i16>,
}

impl JSONNumber {
    pub fn new(mantissa: f64, exponent: Option<i16>) -> Self {
        Self { mantissa, exponent }
    }
}
```

Note: For the sake of simplicity we will display all numbers in the same
internal datatype and ignore some edge cases (e.g. for very large / very
percise numbers)

### Boilerplate & Test Setup

Fine - we have looked deep enough into the JSON spec for right now. But how
should our code behave? Which input should produce what output? Let's write
some tests for this.

I'll introduce a struct called `Lexer` which will later be reponsible to work
on the stream of chars.

```rust {"file": "src/lexer.rs"}
use crate::types::*;
use crate::tokens::*;

#[derive(Debug)]
pub struct Lexer { ... } // Fields omitted for now

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::*;

    macro_rules! lexer_tests { ... }
}
```

> Note that we will use the `lexer_tests` macro later to easily introduce a lot
> of new tests without the syntax garbage that normally would come along with
> that.

Now we start laying out the basic io of our `Lexer` struct; Which types /
traits do we need and what do we want to output? We clearly need some kind of
char stream. Due to the complexity of async code i wont go into the details
of the [stream](https://doc.rust-lang.org/std/stream/index.html) trait now but
they would probably be a great option when implementing something which is
more advanced (like reading from a file or network socket and processing the
input in the same moment).

For now we will just use an peekable iterator of chars - these allow us to look
at the next element of an iterator without directly consuming it.

So we need to provide something which can be turned in an iterator of chars -
like a `String` or a `Vec<char>` etc. and we want to turn it into a
`Vec<Token>` for now.

```rust {"file": "src/lexer.rs", "highlight": [10, 19]}
use std::iter::Peekable;

// omitted

#[derive(Debug)]
pub struct Lexer<I: Iterator<Item = char>> {
    source: Peekable<I>,
}

impl<I> From<I> for Lexer<I>
where
    I: Iterator<Item = char>,
{
    fn from(source: I) -> Self {
        Lexer {
            source: source.peekable(),
        }
    }
}
```

So the implementation of from for our `Lexer` lets us easily create them from
other datatypes such as String through using the `.chars()` method - but mor on
that in a minute. What is still left open is the retrival of tokens. The
smallest step in which we can give out a useful information is by reading a
single token from the input stream.

We can smoothly implement this behavior by using the `Iterator` trait for our
Lexer.

```rust
impl<I> Iterator for Lexer<I>
where
    I: Iterator<Item = char>,
{
    type Item = Token;

    fn next(&mut self) -> Option<Self::Item> {
        None
    }
}
```

One last thing that is holding us back from start implementing the actual lexer
is error handling.. what if we have a completly invalid sequence of chars? We
should notify the person using this that something went wrong - so what error
cases do we have?

- Unexpected EOF
- UnknownChar
- ... // we will expand this list as we proceed handling the 

So lets introduce a enum for our error types. In a more complex environment you
might want to store spans about where the error occured to print out nice error
messages, but for now we will just store that an error occured; not where it was.

```rust {"file": "src/lexer.rs"}
#[derive(Clone, Debug, PartialEq)]
pub enum Error {
    UnexpectedEOF,
    UnknownChar,
}

type Result<T> = std::result::Result<T, Error>;
```

And we will need to update the iterator implementation to yield back results

```rust {"file": "src/lexer.rs", "highlight": [5, 5]}
impl<I> Iterator for Lexer<I>
where
    I: Iterator<Item = char>,
{
    type Item = Result<Token>;

    fn next(&mut self) -> Option<Self::Item> {
        None
    }
}
```

Now we are able to write code that utilizes the iterator behavior; For example
we could just collect into a `Result<Vec<Token>>` instead of inspecting each
token result by itself.

```rust {"file": "src/lexer.rs"}
pub fn lex(source: &String) -> Result<Vec<Token>> {
    Lexer::from(source.chars()).collect()
}
```

We will be primarily using this function to implement our tests for now.  Now
we only need to update our test generation macro to make use of the `lex`
function.

```rust {"file": "src/lexer.rs"}
mod tests {
    ...

    macro_rules! lexer_tests {
        ($($name:ident: $value:expr,)*) => {
            $(
                #[test]
                fn $name() {
                    let (input, expected) = $value;
                    assert_eq!(lex(&input.to_string()), expected);
                }
            )*
        }
    }

    ...
}
```

## Implementing Brackets

So we've got to the point where we can start writing tests and then implement
the `next` function for our `Lexer` struct to pass these. So we start by
writing the most basic tests we could think of when parsing brackets -> single
opening / closing brackets â€“ then we move on to nested brackets to make sure we
do not take the context of a bracket into account when reading it. And finally
we move on to mixed nested combinations. In reality you'd probably want to
write many more tests but to keep the length of thi post somewhat reasonable
we just leave them out for now.

```rust {"file": "src/lexer.rs"}
mod test {
    ...

    mod brackets {
        use super::*;

        lexer_tests! {
            bracket_open: (
                "[",
                Ok(vec![Token::Bracket(ParenthesisType::Open)])
            ),
            bracket_close: (
                "]",
                Ok(vec![Token::Bracket(ParenthesisType::Close)].to_vec())
            ),
            bracket_pair: (
                "[]",
                Ok(vec![Token::Bracket(ParenthesisType::Open), Token::Bracket(ParenthesisType::Close)])
            ),
            bracket_nested: (
                "[[]]",
                Ok(vec![
                    Token::Bracket(ParenthesisType::Open),
                    Token::Bracket(ParenthesisType::Open),
                    Token::Bracket(ParenthesisType::Close),
                    Token::Bracket(ParenthesisType::Close)
                ])
            ),
            bracket_mixed: (
                "[[][",
                Ok(vec![
                    Token::Bracket(ParenthesisType::Open),
                    Token::Bracket(ParenthesisType::Open),
                    Token::Bracket(ParenthesisType::Close),
                    Token::Bracket(ParenthesisType::Open),
                ])
            ),
        }
    }
}
```

Ok.. all tests are red now.. how do we fix this? As always when developing in
a test driven way, we would want to do the minimum work needed to accomplish
our goal. So i guess, let's read the next char from our source and check if
it is an opening / closing bracket, if yes we can categorize them accordingly,
otherwise we just return an error.

> Note that on line 6 we just skip this run if we saw a whitespace. This is an
> important detail, since we want to ignore indentation and formatting of our
> input in general. Languages such as python need to keep track of them but we
> dont.

```rust {"file": "src/lexer.rs"}
    fn next(&mut self) -> Option<Self::Item> {
        match self.source.peek() {
            Some(c) => match c {
                '[' => Some(Ok(Token::Bracket(ParenthesisType::Open))),
                ']' => Some(Ok(Token::Bracket(ParenthesisType::Close))),
                c if c.is_ascii_whitespace() => self.next(),
                _ => Some(Err(Error::UnknownChar)),
            },
            None => None,
        }
    }
```

This looks pretty easy right? Perfect. Then we can move on to the next pair of
tokens.

## Implementing Curly Brackets

So more tokens means more tests. Remember that when writing any kind of lexer,
parser etc. tests must be the heart of your project. The actual code is not
that long most of the time, but unfortunately it is easy to walk into pitfalls
which you might not notice at first. So always always always write **a lot**
of tests.

```rust {"file": "src/lexer.rs"}
mod test {
    ...

    mod curly_brackets {
        use super::*;

        lexer_tests! {
            curly_open: (
                "{",
                Ok(vec![Token::CurlyBracket(ParenthesisType::Open)])
            ),
            curly_close: (
                "}",
                Ok(vec![Token::CurlyBracket(ParenthesisType::Close)])
            ),
            curly_pair: (
                "{}",
                Ok(vec![
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::CurlyBracket(ParenthesisType::Close),
                ])
            ),
            curly_nested: (
                "{{}}",
                Ok(vec![
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::CurlyBracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Close),
                ])
            ),
            curly_mixed: (
                "}}{}",
                Ok(vec![
                    Token::CurlyBracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::CurlyBracket(ParenthesisType::Close),
                ])
            ),
        }
    }

    mod curly_and_normal {
        use super::*;

        lexer_tests! {
            curly_and_normal_mixed: (
                "[}}{[]]}]",
                Ok(vec![
                    Token::Bracket(ParenthesisType::Open),
                    Token::CurlyBracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::Bracket(ParenthesisType::Open),
                    Token::Bracket(ParenthesisType::Close),
                    Token::Bracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Close),
                    Token::Bracket(ParenthesisType::Close),
                ])
             ),
        }
    }
}
```

So to pass 80 lines of tests we need only two more lines of actual code.

```rust {"file": "src/lexer.rs", "highlight": [6, 7]}
    fn next(&mut self) -> Option<Self::Item> {
        match self.source.peek() {
            Some(c) => match c {
                '[' => Some(Ok(Token::Bracket(ParenthesisType::Open))),
                ']' => Some(Ok(Token::Bracket(ParenthesisType::Close))),
                '{' => Some(Ok(Token::CurlyBracket(ParenthesisType::Open))),
                '}' => Some(Ok(Token::CurlyBracket(ParenthesisType::Close))),
                c if c.is_ascii_whitespace() => self.next(),
                _ => Some(Err(Error::UnknownChar)),
            },
            None => None,
        }
    }
```

## Implementing Delimiters

And again add more tests to handle new tokens:

```rust {"file": "src/lexer.rs"}
mod test {
    ...

    mod delimiter {
        use super::*;

        lexer_tests! {
            element_delimiter: (
                ",",
                Ok(vec![Token::ElementDelimiter])
            ),
            element_delimiter_multiple: (
                ",,,,",
                Ok(vec![
                    Token::ElementDelimiter,
                    Token::ElementDelimiter,
                    Token::ElementDelimiter,
                    Token::ElementDelimiter,
                ])
            ),
            key_delimiter: (
                ":",
                Ok(vec![Token::KeyDelimiter])
            ),
            key_delimiter_multiple: (
                ":::",
                Ok(vec![
                    Token::KeyDelimiter,
                    Token::KeyDelimiter,
                    Token::KeyDelimiter,
                ])
            ),
        }
    }
}
```

And we'll add their mapping to the `next` function.

```rust {"file": "src/lexer.rs", "highlight": [8, 9]}
    fn next(&mut self) -> Option<Self::Item> {
        match self.source.peek() {
            Some(c) => match c {
                '[' => Some(Ok(Token::Bracket(ParenthesisType::Open))),
                ']' => Some(Ok(Token::Bracket(ParenthesisType::Close))),
                '{' => Some(Ok(Token::CurlyBracket(ParenthesisType::Open))),
                '}' => Some(Ok(Token::CurlyBracket(ParenthesisType::Close))),
                ',' => Some(Ok(Token::ElementDelimiter)),
                ':' => Some(Ok(Token::KeyDelimiter)),
                c if c.is_ascii_whitespace() => self.next(),
                _ => Some(Err(Error::UnknownChar)),
            },
            None => None,
        }
    }
```

Everything is green again? Alright; Lets move on!

## Implementing Numbers

So this is where it starts to get a little more complicated than just reading a
single char from our source and map it into a token symbol. Remember the rules
mentioned in the JSON spec?

> A series of digits (without leading zeros!) optionally prefixed by a `-`,
> optionally delimited by a `.`.  At the end of a number literal we can specify
> an exponent with `e` or `E` followed by an optional sign `+` or `-` and  a
> series of digits (may contain leading zeros).

So this sounds like a lot of work to do, but it turns out this is fairly
managable aswell. But first things first â€“ the tests will show us how our code
should behave:

```rust {"file": "src/lexer.rs"}
mod test {
    ...

    mod number {
        use super::*;

        lexer_tests! {
            number_zero: (
                "0",
                Ok(vec![Token::Number(JSONNumber::new(0.0, None))])
            ),
            number_multiple: (
                "0 1 2",
                Ok(vec![
                    Token::Number(JSONNumber::new(0.0, None)),
                    Token::Number(JSONNumber::new(1.0, None)),
                    Token::Number(JSONNumber::new(2.0, None)),
                ])
            ),
            number_long: (
                "1000000",
                Ok(vec![Token::Number(JSONNumber::new(1000000.0, None))])
            ),
            number_no_exponent: (
                "4",
                Ok(vec![Token::Number(JSONNumber::new(4.0, None))])
            ),
            number_unsigned_exponent: (
                "4E2",
                Ok(vec![Token::Number(JSONNumber::new(4.0, Some(2)))])
            ),
            number_unsigned_long_exponent: (
                "4E200",
                Ok(vec![Token::Number(JSONNumber::new(4.0, Some(200)))])
            ),
            number_unsigned_padded_exponent: (
                "4E000002",
                Ok(vec![Token::Number(JSONNumber::new(4.0, Some(2)))])
            ),
            number_pos_signed_padded_exponent: (
                "4E+000002",
                Ok(vec![Token::Number(JSONNumber::new(4.0, Some(2)))])
            ),
            number_signed_exponent: (
                "4E-2",
                Ok(vec![Token::Number(JSONNumber::new(4.0, Some(-2)))])
            ),
            number_signed_padded_exponent: (
                "4E-000002",
                Ok(vec![Token::Number(JSONNumber::new(4.0, Some(-2)))])
            ),
            float: (
                "14.0",
                Ok(vec![Token::Number(JSONNumber::new(14.0, None))])
            ),
            float_multiple: (
                "0.1 12.5 2.12",
                Ok(vec![
                    Token::Number(JSONNumber::new(0.1, None)),
                    Token::Number(JSONNumber::new(12.5, None)),
                    Token::Number(JSONNumber::new(2.12, None)),
                ])
            ),
            float_multiple_exp: (
                "0.1E1 12.5E-0002 2.12E213",
                Ok(vec![
                    Token::Number(JSONNumber::new(0.1, Some(1))),
                    Token::Number(JSONNumber::new(12.5, Some(-0002))),
                    Token::Number(JSONNumber::new(2.12, Some(213))),
                ])
            ),
            float_long: (
                "10000000000000000.0",
                Ok(vec![Token::Number(JSONNumber::new(10000000000000000.0, None))])
            ),
            float_complex: (
                "214.12498",
                Ok(vec![Token::Number(JSONNumber::new(214.12498, None))])
            ),
            float_signed_complex: (
                "-214.12498",
                Ok(vec![Token::Number(JSONNumber::new(-214.12498, None))])
            ),
            float_signed_exp: (
                "-214.12498E+001",
                Ok(vec![Token::Number(JSONNumber::new(-214.12498, Some(1)))])
            ),
            float_signed_negative_exp: (
                "-214.12498E-200",
                Ok(vec![Token::Number(JSONNumber::new(-214.12498, Some(-200)))])
            ),
            float_unsigned_exp: (
                "2.0E2",
                Ok(vec![Token::Number(JSONNumber::new(2.0, Some(2)))])
            ),
            invalid_float_many_decimal_points: (
                "20.0.0.0",
                Err(Error::InvalidNumberFormat)
            ),
            invalid_number: (
                "+2",
                Err(Error::InvalidNumberFormat)
            ),
        }
    }
}
```

Ok we should have a lot of tests that are red now. So lets start changing that
by introducing a generic helper function called `read_while` to read all
characters of a certain type. We will need to create a seperate impl block for
this function since it is not a member of the `Iterator` trait.

So how can we come up with the function signature for this one? We need a
mutable reference to the lexer since we want to modify the underlying iterator
(hence the `&mut self`) and we want to take a generic predicate function which
indicates if a char should be read or if we can stop. This one needs to be
callabe with a `char` and should return a `bool` (hence the `P: Fn(char) ->
bool`) and finally we want to return the string we have read from our source.

```rust {"file": "src/lexer.rs"}
impl<I> Lexer<I>
where
    I: Iterator<Item = char>,
{
    fn read_while<P>(&mut self, predicate: P) -> String
    where
        P: Fn(char) -> bool,
    {
        None
    }
}
```

But unfortunately we have no implementation at the moment; So we want to move
all characters which fulfill the predicate in a string and stop if one doesn't.
It is important for us to not consume any chars that we don't want to read now,
so we will need to make use of `peek()` to look ahead.


```rust {"file": "src/lexer.rs"}
impl<I> Lexer<I>
where
    I: Iterator<Item = char>,
{
    fn read_while<P>(&mut self, predicate: P) -> String
    where
        P: Fn(char) -> bool,
    {
        let mut res = String::new();

        while let Some(next) = self.source.peek() {
            match *next {
                next if predicate(next) => {
                    res.push(self.source.next().unwrap());
                }
                _ => break,
            }
        }

        res
    }
}
```

This one should work for now. One last thing we need to do is we need to add 2
new error types to our error enum:

```rust {"file": "src/lexer.rs", "highlight": [5, 6]}
#[derive(Clone, Debug, PartialEq)]
pub enum Error {
    UnexpectedEOF,
    UnknownChar,
    InvalidNumberFormat,
    InvalidExponentFormat,
}
```
Now we can focus again on implementing the number detection; We know that a
number starts either with `0-9` or a `-` hence we can add this to our match
statement in `Iterator::next`. Note that we also match a `+` sign to notify the
caller about an invalid number.

```rust {"file": "src/lexer.rs"}
                c if c.is_ascii_digit() || c == '-' || c == '+' => {
```

The very first thing which we want to do in this branch is do a simple check for leading zeros or a leading `+` sign. Both cause the error `InvalidNumberFormat` for now.

```rust {"file": "src/lexer.rs"}
                    {
                        let err = match c {
                            '0' => match self.source.peek() {
                                Some(n) if n.is_ascii_digit() => Some(Error::InvalidNumberFormat),
                                _ => None,
                            },
                            '+' => Some(Error::InvalidNumberFormat),
                            _ => None,
                        };

                        if let Some(e) = err {
                            self.read_while(|next| match next {
                                ',' | ']' | '}' => false,
                                _ => true,
                            });

                            return Some(Err(e));
                        }
                    }
```

Now that we know that we know that we are in the context of consuming a number;
So lets read anything which is an ascii digit or a `.` as the mantissa. We do
not need to care about when or how often the `.` occurs because rust has
already a nice implementation for that in `std::string` called `parse`.

So the mantissa part is kind of simple to do using our `read_while` helper:

```rust {"file": "src/lexer.rs"}
                    let mantissa = {
                        let mut rest = self.read_while(|next| next.is_ascii_digit() || next == '.');
                        rest.insert(0, c);
                        rest
                    };
```

Now we are left with the question if we have an exponent and if yes, we need to
read it accordingly. So we remember from the spec that either `e` or `E`
denotes the start of an exponent. If we saw one, we can skip that char since it
was nothing more than a hint. Now we again read up everything that is either a
digit, a `+` or a `-` (remember that the exponent is allowed to have the prefix
`+`). If we have read everything and we have an empty exponent we are in an
invalid state and want to mark this token as failed. Otherwise we can return
it.

```rust {"file": "src/lexer.rs"}
                    let exponent: Option<String> = match self.source.peek() {
                        Some('e') | Some('E') => {
                            self.source.next();

                            let exp = self.read_while(|next| {
                                next.is_ascii_digit() || next == '+' || next == '-'
                            });

                            if exp == "+" || exp == "-" || exp.is_empty() {
                                return Some(Err(Error::InvalidExponentFormat));
                            }

                            Some(exp)
                        }
                        _ => None,
                    };
```

Now the core actions are happening in the `parse` method and we will
just need to evaluate the results of that call:

```rust {"file": "src/lexer.rs"}
                    let parsed_mantissa = match mantissa.parse::<f64>() {
                        Ok(num) => num,
                        Err(_) => return Some(Err(Error::InvalidNumberFormat)),
                    };

                    let parsed_exponent = match exponent.map(|e| e.parse::<i16>()) {
                        Some(Err(_)) => return Some(Err(Error::InvalidNumberFormat)),
                        Some(Ok(exponent)) => Some(exponent),
                        None => None,
                    };
```

Finally if everything was valid we can simply construct our number token from
the variables above.

```rust {"file": "src/lexer.rs"}
                    Some(Ok(Token::Number(JSONNumber::new(
                        parsed_mantissa,
                        parsed_exponent,
                    ))))
                }
```

## Implementing Null and Booleans

The reason we can do both in the same block of code is that both types of
tokens only consists of keywords (`null` / `true` / `false`). This means that
we can read a whole keyword and just check if we know it, otherwise we notify
the user through a new error `UnkownKeyword`.

```rust {"file": "src/lexer.rs", "highlight": [5, 5]}
#[derive(Clone, Debug, PartialEq)]
pub enum Error {
    UnexpectedEOF,
    UnknownChar,
    UnknownKeyword,
    InvalidNumberFormat,
    InvalidExponentFormat,
}
```

So lets write some tests for these tokens:

```rust {"file": "src/lexer.rs"}
mod test {
    ...
    mod null {
        use super::*;

        lexer_tests! {
            null: (
                "null",
                Ok(vec![Token::Null])
            ),
            null_multiple: (
                "null null null",
                Ok(vec![
                    Token::Null,
                    Token::Null,
                    Token::Null
                ])
            ),
            null_typo: (
                "nulll",
                Err(Error::UnkownKeyword)
            ),
        }
    }

    mod boolean {
        use super::*;

        lexer_tests! {
            true_single: (
                "true",
                Ok(vec![Token::Boolean(true)])
            ),
            true_multiple: (
                "true true true",
                Ok(vec![
                    Token::Boolean(true),
                    Token::Boolean(true),
                    Token::Boolean(true),
                ])
            ),
            true_typo: (
                "treu",
                Err(Error::UnkownKeyword)
            ),
            false_single: (
                "false",
                Ok(vec![Token::Boolean(false)])
            ),
            false_multiple: (
                "false false false",
                Ok(vec![
                    Token::Boolean(false),
                    Token::Boolean(false),
                    Token::Boolean(false),
                ])
            ),
            false_typo: (
                "fales",
                Err(Error::UnkownKeyword)
            ),
            mixed_easy: (
                "false true",
                Ok(vec![
                    Token::Boolean(false),
                    Token::Boolean(true),
                ])
            ),
            mixed_complex: (
                "false true false true false",
                Ok(vec![
                    Token::Boolean(false),
                    Token::Boolean(true),
                    Token::Boolean(false),
                    Token::Boolean(true),
                    Token::Boolean(false),
                ])
            ),
            mixed_typo: (
                "false treu false true false",
                Err(Error::UnkownKeyword)
            ),
        }
    }
}
```

Through using the builtin method `is_ascii_alphabetic` and our own helper
`read_while` things become really easy to implement; We just add another
pattern to our match statement to check if we have a alphabetic char - if yes
we can read all following and match them:

```rust {"file": "src/lexer.rs"}
                c if c.is_ascii_alphabetic() => {
                    let keyword = {
                        let mut rest = self.read_while(|next| next.is_ascii_alphabetic());
                        rest.insert(0, c);
                        rest
                    };
```

After reading the whole keyword we just transform it into its according token.

```rust {"file": "src/lexer.rs"}
                    match keyword.as_str() {
                        "true" => Some(Ok(Token::Boolean(true))),
                        "false" => Some(Ok(Token::Boolean(false))),
                        "null" => Some(Ok(Token::Null)),
                        _ => Some(Err(Error::UnkownKeyword)),
                    }
                }
```

And boom - we have implemented simple keyword lexing - all tests should pass
now.

## Implementing Strings

So one last time we need to adjust our error type since we need to mark strings
as unclosed now:

```rust {"file": "src/lexer.rs", "highlight": [6, 6]}
#[derive(Clone, Debug, PartialEq)]
pub enum Error {
    UnexpectedEOF,
    UnknownChar,
    UnkownKeyword,
    UnclosedString,
    InvalidNumberFormat,
    InvalidExponentFormat,
}
```

Then we can write a bunch of tests for strings:

```rust {"file": "src/lexer.rs"}
mod test {
    ...

    mod strings {
        use super::*;

        lexer_tests! {
            string: (
                r#""foo""#,
                Ok(vec![Token::String("foo".to_string())])
            ),
            string_unclosed: (
                r#""foo"#,
                Err(Error::UnclosedString)
            ),
            string_contains_control_chars: (
                r#""a',b;c""#,
                Ok(vec![Token::String("a',b;c".to_string())])
            ),
            string_multiple: (
                r#""foo" "bar""#,
                Ok(vec![
                    Token::String("foo".to_string()),
                    Token::String("bar".to_string())
                ])
            ),
            string_with_spaces: (
                r#"" foo   ""#,
                Ok(vec![Token::String(" foo   ".to_string())])
            ),
            string_multiple_with_spaces: (
                r#"" foo   " "bar" " baz" "#,
                Ok(vec![
                    Token::String(" foo   ".to_string()),
                    Token::String("bar".to_string()),
                    Token::String(" baz".to_string())
                ])
            ),
        }
    }

    ...
}
```

> *Disclaimer:* Due to the complexity that comes along with handling escape
> sequences we will leave them out. But if you want to dig into unicode
> support, the crate
> [unicode-segmentation](https://crates.io/crates/unicode-segmentation) looks
> really promising for that.

Now as usual we can proceed with the implementation: So when do we want to read
a string? We know from the spec that strings are enclosed by double quotes, so
we want to start when we see the first `"` and we want to read everything until
we see the next `"`.

By ignoring the escaping, our code becomes really simple and straight forward:

```rust {"file": "src/lexer.rs"}
                '"' => {
                    let string = self.read_while(|next| next != '"');

                    match self.source.next() {
                        Some('"') => Some(Ok(Token::String(string))),
                        _ => Some(Err(Error::UnclosedString)),
                    }
                }
```

We read everything that occurs after the first `"` and stop again when we see
the second `"`; This means we either stopped because no further chars were available (this would mean we have an unclosed string) or we stopped because we saw the second double quote which indicates that we can return the token.

Now all of our tests should be green again. If you found yourself confused by
the amount of smaller code snippets: The whole code for this post is available
[here](https://github.com/schulke-214/maximilian-schulke/tree/main/content/articles/2021-07-14-implementing-a-json-lexer-in-rust/code).

## Putting it all together

Up to this point we have only wrote smaller token type based tests - but in a
real usecase we'd want to detect a bunch of different tokens in a complex
order. So lets write some tests where we will tokenize whole json blobs to see
if everything is correct.

```rust {"file": "src/lexer.rs"}
mod tests {
    ...

    mod blobs {
        use super::*;

        lexer_tests! {
            simple: (
                r#"{"foo": "bar"}"#,
                Ok(vec![
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::String("foo".to_string()),
                    Token::KeyDelimiter,
                    Token::String("bar".to_string()),
                    Token::CurlyBracket(ParenthesisType::Close),
                ])
            ),
            single_nested: (
                r#"{"foo": "bar", "baz": [2.0, true, false, null]}"#,
                Ok(vec![
                    Token::CurlyBracket(ParenthesisType::Open),
                    Token::String("foo".to_string()),
                    Token::KeyDelimiter,
                    Token::String("bar".to_string()),
                    Token::ElementDelimiter,
                    Token::String("baz".to_string()),
                    Token::KeyDelimiter,
                    Token::Bracket(ParenthesisType::Open),
                    Token::Number(JSONNumber::new(2.0, None)),
                    Token::ElementDelimiter,
                    Token::Boolean(true),
                    Token::ElementDelimiter,
                    Token::Boolean(false),
                    Token::ElementDelimiter,
                    Token::Null,
                    Token::Bracket(ParenthesisType::Close),
                    Token::CurlyBracket(ParenthesisType::Close),
                ])
            ),
            array: (
                r#"["foo", "bar", "baz"]"#,
                Ok(vec![
                    Token::Bracket(ParenthesisType::Open),
                    Token::String("foo".to_string()),
                    Token::ElementDelimiter,
                    Token::String("bar".to_string()),
                    Token::ElementDelimiter,
                    Token::String("baz".to_string()),
                    Token::Bracket(ParenthesisType::Close),
                ])
            ),
        }
    }

    ...
}
```

If you followed along correctly all tests should immediatly pass ðŸ˜€

I think you get the point; **In reality you should write many more tests** but
this would increase the length of this posts by an unreasonable amount.

Furthermore we should write a bunch of tests for invalid / untokenizable input
to make sure our errors come out as expected.

## Summary / What next?

Now we can tokenize nearly every json blob (except those with escaped strings);
We could proceed by implementing a parser using this implementation to check
if the token occur in a valid order; But thats a topic for another post.

The underlying is kind of similar to the lexer but when parsing we do only look
at tokens and not chars. And the result of parsing is an `AST` and not a
`Vec<Token>`.

So if you're curious you could go ahead and try to build a parser yourself!

If you've found any errors in this post [please raise an
issue](https://github.com/schulke-214/maximilian-schulke)!
